\chapter{\colorbox{cyan}{Análise da eficiência de algoritmos}}
\label{analise-eficiencia-de-algoritmos}

Esse capítulo apresenta um roteiro para análise da eficiência de
algoritmos e apresenta exemplos simples de problemas e de soluções
usando esse roteiro.

Além da eficiência, algoritmos podem ser analisados quanto a
facilidade de mostrar ou provar correção, simplicidade e
generalidade. 

Ao contrário da análise da eficiência, simplicidade e facilidade de
mostrar correção são critérios bastante subjetivos. É bastante difícil
estabelecer métricas para tais critérios. Generalidade, por sua vez,
pode ser medida pelo tamanho do domínio da entrada do problema
resolvido, mas há situações em que o desenvolvimento de um algoritmo
mais geral é desnecessário (pouco vantajoso) ou difícil, e tal
dificuldade ou necessidade é difícil de ser medida precisamente.

O projeto de algoritmos envolve revisão e busca de melhorias, com o
qual programadores devem se envolver. 

Em geral, o projeto de algortimos envolve a adoção de soluções que
favorecem um aspecto em detrimento de outro, e um aspecto que costuma
ser bastante influente é o tempo disponível para desenvolvimento do
programa. O desenvolvimento de algoritmos {\em ótimos\/} é uma questão
relativa ao {\em problema\/} que está sendo resolvido e, mesmo
restringindo ao aspecto de eficiência, para muitos problemas saber
dizer qual é o algoritmo ótimo é difícil e muitas vezes não tem uma
resposta conhecida. Vamos falar mais sobre esse assunto na seção
{P-vs-NP}.

% Anany Levitin cita \ref{Anany-Levitin-analysis-and-design-of-algs} a
% seguinte observação de Saint-Exupéry (que ele tirou de citação feita
% por Jon Bentley): ``Um projetista sabe que chegou à perfeição não
% quando não há mais nada a incluir, mas quando não há mais nada a
% remover''.

A prova de correção de programas é uma área da ciência da computação
que está em franca evolução, atualmente. O desenvolvimento de teorias
de tipos \cite{Sorensen98lectureson}, baseadas nos chamados ``tipos
dependentes'' \cite{Bove:2009:DTW,Nederpelt-Geuvers-2014}, tem
evoluído bastante. Esse desenvolvimento tem estimulado trabalhos com
os chamados ``assistentes de prova''
\cite{Geuvers2009:Proof-assistants}. Esses programas e linguagens, no
entanto, ainda requerem bastante treinamento e parecem ainda estar em
processo de evolução, antes que possam ser mais amplamente
usados. Atualmente, a correção da vasta maioria dos programas usados
na prática não é demonstrada, mas sujeita a testes. Provas de correção
e técnicas de teste de programas não fazem parte do escopo deste
livro; no entanto, vamos usar provas de indução e definição de
invariantes para mostrar informalmente a correção de programas.

Como usualmente, não é feita neste livro nenhuma {\em validação dos
  dados de entrada}, isto é, não é verificado que os dados de entrada
realmente estão dentro dos limites estabelecidos no enunciado de um
problema. Em programas usados na prática, essa verificação deve ser
incluída (o enunciado do problema não deve estabelecer tais limites
para os dados de entrada), mas em geral essa validação não envolve
nenhum aspecto mais relevante para a tarefa de programação (apenas
inclusão de testes para emissão de mensagens de erro no caso em que os
dados de entrada não satisfaçam a esses testes).

As seções a seguir apresentam alguns exemplos de problemas para os
quais há vamos analisar a eficiência de um algoritmo simples que os
resolve. O roteiro para análise da eficiência é o seguinte:

\begin{enumerate}

\item Determinar variável ($n$) que representa o tamanho dos dados de
  entrada.

\item Identificar operações que vão determinar a variação na
  eficiência do programa durante a execução.

\item Expresse o número de vezes que a operação é executada em função
  de $n$, chamada de expressão-determinante-da-eficiência.

\item Resolva ou simplifique a expressão-determinante-da-eficiência.

\end{enumerate}

No caso de um programa recursivo, a
expressão-determinante-da-eficiência é em geral uma expressão escrita
em função de $T(n-1)$ ou outro argumento da função $T$ (de tempo de
execução) menor que $n$, que expressa $T(n)$ de modo recursivo,
criando o que é chamado de uma {\em relação de recorrência}.

\begin{quotation}
  {\em Uma relação de recorrência é uma definição recursiva para a
    qual busca-se uma solução não recursiva que a simplifique, que
    especifica a mesma relação.\/}
\end{quotation}

Note: Essa busca pode não ter sucesso, isto é, existem equações
recursivas que não podem ser simplificadas para uma solução não
recursiva (de fato, isto acontece com a maioria das equações
recursivas). Mas as equações que expressam tempos de execução de
pequenos programas em geral são bastante simples e podem de fato ser
simplificadas para uma equação não recursiva.

No caso de um programa não recursivo, a
expressão-determinante-da-eficiência é em geral um somatório, que em
geral também pode ser simplificado.

As seções seguintes apresentam exemplo de problemas simples e suas
soluções, para os quais a eficiência é analisada usando o roteiro
acima.

\section{Número de Dígitos}
\label{numero-de-digitos}

O problema é determinar o número de dígitos de um número em uma dada
base usada para representação desse número. O número e a base são
dados de entrada.

\subsection{Versão funcional}

\newcommand{\numDigs}{{\it numDigs\/}}

A versão funcional é apresentada em Haskell a seguir:

\progb{
\numDigs\ $x$ $b$\\
  \hspace*{.2cm} | $x$ < $b$   = 1\\
  \hspace*{.2cm} | \otherwise\ = 1 + \numDigs\ ($x$ `\ddiv` $b$)
}

A variável que representa o tamanho dos dados de entrada é igual a
$n$.  A variação do tempo de execução $T(n)$ é dada por (considerando
como $k$ uma constante igual ao tempo gasto pela operação de somar 1 a
um valor qualquer mais o tempo gasto pela operação de comparar o
argumento $x$ com $b$):

 \[ \begin{array}{lll}
       T(n) & = 0                  & \text{ se } n < b\\
       T(n) & = T(n `\ddiv` b) + k & \text{ caso contrário}
    \end{array}
 \]
Vamos considerar que $n$ é uma potência de $b$ --- isto é, $n = b^i$,
para algum $i\geq 0$. Essa consideração é baseada na regra .... \ldots

Para $i\geq b$, obtemos:  
 \[ \begin{array}{ll}
       T(b^i) & = T(b^{i-1}) + k \\
              & = T(b^{i-2}) + (2 \times k) \\
              & \ldots
    \end{array}
 \]
Para $n=b^i$, obtemos $T(b^i) = T(b^0) + (i\times k) = i\times k$.
Portanto, $T(n) = log_b (i\times k)$ e portanto $T(n) \asymp lg n$.

\subsection{Versão imperativa}

\newcommand{\numD}{{\it numD\/}}

A versão imperativa é similar, usando um comando de repetição em vez
de recursão:

\progb{
\numDigs\ ($n$,$b$) \\
  \hspace*{.2cm} \numD\ = 0\\
  \hspace*{.2cm}   \while\ ($n$ > $b$) \\
       \hspace*{1cm} \numD\ = \numD\ + 1\\
       \hspace*{1cm} $n$ = $n$ / $b$
}
A expressão-determinante-da-eficiência é igual a $m \times \Theta(1)$,
onde $m$ é o número de vezes que o comando de repetição é executado e
$\Theta(1)$ expressa o tempo gasto nos comandos internos ao comando de
repetição. Como a variável $n$ recebe, a cada repetição, o valor do
quociente da divisão do valor de $n$ (anterior à atribuição) por $b$,
obtemos: $T(n) \asymp m \asymp log_b n \asymp lg n$. 

Note que $T(n) \asymp lg n$ para qualquer base $b$.

Note também que $T(n)$ (e o número de repetições no \while) aumenta
logaritmicamente com um aumento (linear) no {\em valor\/} de $n$, mas
aumenta linearmente com um aumento no número de dígitos de $n$ (o
valor de $n$ aumenta exponencialmente com um aumento no número de
dígitos de $n$).

\section{Maior Elemento}
\label{maior-elemento}

\newcommand{\maxElem}{{\it maxElem\/}}

Considere o problema de encontrar o maior elemento de uma lista. 

A versão funcional apresentada abaixo simplesmente usa \foldl': 

\progb{\maxElem\ :: \Ord\ $a$ => [$a$] -> $a$\\
\maxElem\ ($a$:$x$) = \foldl' \max\ $a$ $x$ }

A função \max, definida no prelúdio de Haskell, retorna o maior entre
dois valores, passados como parâmetros:

\progb{\max\ :: \Ord\ $a$ => $a$ -> $a$ -> $a$\\
       \max\ $a$ $b$ \\
         \hspace*{.2cm} | $a$ <= $b$  = $b$
         \hspace*{.2cm} | \otherwise\ = $a$
      }
 
A função \foldl\ é definida no prelúdio de Haskell como a seguir:

\progb{
\foldl\ $f$ $z$ [] \hspace*{1cm} = $z$\\
\foldl\ $f$ $z$ ($a$:$x$) = \foldl\ $f$ ($f$ $z$ $a$) $x$
}

A função \foldl\, aplicada a uma função binária $f$, um valor inicial
$z$ e uma lista, ``reduz'' (em geral, mas mais precisamente
transforma) a lista usando a função $f$ da esquerda para a direita
(daí o nome \foldl: o {\it l\/} é de {\em {\underline{l}eft\/}}, em
português {\em esquerda\/}):

  \[ \text{\tt{ \foldl\ $f$ $z$ [$e_1$, $e_2$, \ldots, $e_n$] == (\ldots (($z$ `$f$` $e_1$) `$f$` $e_2$) `$f$`\ldots) `$f$` $e_n$}} \]

A função \foldl'\ se comporta de modo similar a \foldl, mas é ``menos
preguiçosa'': \foldl' $f$ força a avaliação de $f$, de modo que, {\tt
  $z$ `$f$` $e_1$} é avaliado antes que a expressão ($z$ `$f$` $e_1$)
`$f$` $e_2$) seja formada, e assim sucessivamente. Com \foldl, toda a
expressão {\tt (\ldots (($z$ `$f$` $e_1$) `$f$` $e_2$) `$f$`\ldots)
  `$f$` $e_n$} é construída antes da avaliação de {\tt $z$ `$f$`
  $e_1$}. O uso de \foldl'\ é adequado quando a função $f$ é estrita
(ou seja, quando $f$ não é preguiçosa), ou, ainda, quando a avaliação
de $z$ `$f$` $e_1$ requer a avaliação de $e_1$.

\HRule
{\em Nota\/}: 

Uma função $f$ é dita estrita se (escrevendo sucintamente) $f\: \bot =
\bot$, ou seja: quando o resultado de aplicar $f$ a um argumento que
fica em ciclo infinito faz com que a chamada a $f$ fique em ciclo
infinito. O valor $\bot$ é usado para indicar ``ciclo infinito'', e
também ocorrência de erro devido a recursos, em quantidade finita,
serem consumidos para avaliação, durante a execução. 

Em Haskell, ao contrário da grande maioria das linguagens de
programação, a estratégia de avaliação de expressões é ``preguiçosa''
(em inglês, ``lazy''). Isso significa que o argumento para uma função
não é avaliado quando a expressão é chamada, mas simplesmente
substituído pelo parâmetro no corpo da função, para posterior
avaliação, se necessário. Além disso, se for necessária, a avaliação
do argumento só é feita uma única vez, na estratégia de avaliação
preguiçosa; nas outras vezes em que o argumento for usado, é usado o
valor resultante da avaliação feita na primeira vez.

Na maioria das linguagens de programação, a estratégia de avaliação de
expressões é ``gulosa'' (em inglês, ``eager''). Nessa estratégia, o
argumento é avaliado antes de uma chamada à função. Essa estratégia
faz com que todas as funções sejam estritas.

As diferenças resultantes do uso de estratégias de avaliação
preguiçosa e gulosa estão fora do escopo deste livro. 

No entanto, vale observar que o uso de \foldl' neste exemplo não
altera a complexidade assintótica do tempo de execução, mas pode
afetar bastante o espaço necessário e a constante de proporcionalidade
da função que expressa o tempo de execução.

\HRule

A relação de recorrência é 

 \begin{equation}
    T(n) = T(n-1) + k 
    \label{recorrencia1}
 \end{equation}
onde $n$ é o tamanho (número de elementos) da lista e $k$ é uma
constante que expressa o tempo de execução da aplicação da função
$\max$ a dois valores inteiros (informalmente, costuma-se dizer: ``$k$
é o tempo de execução da função \max, quando na verdade se quer dizer
o tempo de execução da aplicação da função a argumentos, que podem ser
quaisquer). No caso base, temos $T(0) = 0$.

Essa relação de recorrência tem solução fácil. Vamos mostrar sua
solução pelo {\em
  método-de-substituição-para-eliminação-da-recursão\/} ilustrado a
seguir. 

Temos: $T(n-1) = T(n-2) + k$. Substituindo $T(n-1)$ na relação de
recorrência (\ref{recorrencia1}), obtemos: $T(n) = (T(n-2) + k) + k =
T(n-2) + 2\times k$. É fácil ver que, para todo $i=1,\ldots,n$, temos:
$T(n) = T(n-i) + i\times k$. Para $i=n$, temos: $T(n) = n\times k$ e,
portanto, $T(n) = \Omega(n)$.


Note que nem é preciso o método para concluir que $T(n) = n\times k$;
basta raciocinar sobre a definição de $T$: 

    \[ \begin{array}[t]{ll}
         T(n) = 0          & \text{ se } n = 0\\
         T(n) = T(n-1) + k & \text{ caso contrário}
       \end{array}
    \]

A expressão que define $T(n)$ é igual a 0 quando $n$ é igual a 0
aumenta de $k$ qundo $n$ aumenta de 1; ou seja, é uma definição de
$n\times k$.

O que chamamos de expressão-determinante-da-eficiência pode ser
expresso por $T(n-1) + k$, ou (depois de resolvida a relação de
recorrência) por $n\times k$.

A complexidade é, assim, a mesma da pesquisa sequencial em uma lista:
$O(n)$ no pior caso. Informalmente, o raciocínio pode ser similar ao
seguinte: o tempo de execução é $O(n)$ pois o custo das operações
realizadas em chamada recursiva é constante ($O(1)$) e o número de
chamadas recursivas é $O(n)$ (cresce linearmente com $n$).

\subsection{Versão imperativa}

\ldots parei aqui......

\section{Unicidade de Elementos}

\section{Multiplicação de Matrizes}

\section{Números de Fibonacci}
