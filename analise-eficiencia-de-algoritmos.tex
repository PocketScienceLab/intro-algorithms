%!TEX encoding = ISO-8859-1
\chapter{Análise da eficiência de algoritmos}
\label{analise-eficiencia-de-algoritmos}

Esse capítulo apresenta um roteiro para análise da eficiência de
algoritmos, juntamente com exemplos simples de problemas e suas soluções,
usando esse roteiro.

Além da eficiência, algoritmos podem ser analisados quanto a
facilidade de mostrar ou provar correção, simplicidade e
generalidade. Ao contrário da análise da eficiência, simplicidade e facilidade de
mostrar correção são critérios bastante subjetivos. É bastante difícil
estabelecer métricas para tais critérios. Generalidade, por sua vez,
pode ser medida pelo tamanho do domínio da entrada do problema
resolvido, mas há situações em que o desenvolvimento de um algoritmo
mais geral é desnecessário (pouco vantajoso) ou difícil, e tal
dificuldade ou necessidade é difícil de ser medida precisamente.

Em geral, o projeto de algortimos envolve a adoção de soluções que
favorecem um aspecto em detrimento de outro, e um aspecto que costuma
ser bastante influente é o tempo disponível para desenvolvimento do
programa. O desenvolvimento de algoritmos {\em ótimos\/} é uma questão
relativa ao {\em problema\/} que está sendo resolvido e, mesmo
restringindo ao aspecto de eficiência, para muitos problemas saber
dizer qual é o algoritmo ótimo é difícil e muitas vezes não tem uma
resposta conhecida. 

%Vamos falar mais sobre esse assunto na seção \ref{sec:P-vs-NP}.

\HRule
{\em Nota\/}: 

A prova de correção de programas é uma área da ciência da computação
que está em franca evolução, atualmente. O desenvolvimento de teorias
de tipos \cite{Sorensen98lectureson}, baseadas nos chamados ``tipos
dependentes'' \cite{Bove:2009:DTW,Nederpelt-Geuvers-2014}, tem
evoluído bastante. Esse desenvolvimento tem estimulado trabalhos com
os chamados ``assistentes de prova''
\cite{Geuvers2009:Proof-assistants}. Esses programas e linguagens, no
entanto, ainda requerem bastante treinamento e parecem ainda estar em
processo de evolução, antes que possam ser mais amplamente
usados. Atualmente, a correção da vasta maioria dos programas usados
na prática não é demonstrada, mas sujeita a testes. Provas de correção
e técnicas de teste de programas não fazem parte do escopo deste
livro; no entanto, vamos usar provas de indução e definição de
invariantes para mostrar informalmente a correção de programas.

\HRule

Como usualmente, vamos em geral omitir a {\em validação dos dados de
  entrada}, isto é, não verificar se os dados de entrada realmente
estão dentro dos limites estabelecidos no enunciado de um problema. Em
programas usados na prática, essa verificação deve ser incluída mas,
em geral, essa validação não envolve nenhum aspecto mais relevante
para a tarefa de programação, envolvendo apenas a inclusão de testes
para emissão de mensagens de erro no caso em que os dados de entrada
não satisfaçam a esses testes.

As seções a seguir apresentam alguns exemplos de problemas e
algoritmos simples para solução desses problemas, seguidos da análise
de eficiência desses algoritmos. O roteiro para análise da eficiência
é o seguinte:

\begin{enumerate}

\item Determinar a variável ($n$) que representa o tamanho dos dados de
  entrada.

\item Identificar operações que são relevantes para determinar a 
  eficiência do programa durante a execução.

\item Expressar o número de vezes que essas operações são executadas,
  em função de $n$, chamada de expressão-determinante-da-eficiência.

\item Resolver ou simplificar a expressão-determinante-da-eficiência.

\end{enumerate}

No caso de um programa recursivo, a
expressão-determinante-da-eficiência é, em geral, também definida
recursivamente. Isto é, o tempo de execução do algoritmo para uma
entrada de tamanho $n$ -- $T(n)$ -- é escrito em termos de $T(n-1)$,
ou em termos de $T(k)$, para algum $k<n$. Dizemos, nesse caso, que
$T(n)$ é definido por uma {\em relação de recorrência}.

\begin{quotation}
  {\em Uma relação de recorrência é uma definição recursiva para a
    qual busca-se uma solução não recursiva que a simplifique, que
    especifica a mesma relação.\/}
\end{quotation}

Existem inúmeras equações recursivas para as quais não podemos
encontrar uma solução não recursiva (de fato, isto acontece com a
maioria das equações recursivas). Entretanto, para os problemas com os
quais vamos lidar a seguir, as equações recursivas que expressam o
tempos de execução de algoritmos para solução desses problemas podem
ser resolvidas de maneira bastante simples.

No caso de um programa não recursivo, a
expressão-determinante-da-eficiência é em geral um somatório, que
muitas vezes também pode ser simplificado, usando propriedades de
somatórios como as seguintes, onde $a,b,c$ são constantes:

  \[ \begin{array}{l}
       \sum_{i=a}^{b} cm_i = c \sum_{i=a}^{b} m_i\\
       \sum_{i=a}^{b} (m_i + n_i) = \sum_{i=a}^{b} m_i + \sum_{i=a}^{b} n_i\\
       \sum_{i=a}^{b} (m_i - n_i) = \sum_{i=a}^{b} m_i - \sum_{i=a}^{b} n_i
     \end{array}
  \]
O Apêndice \ref{Somatorios} apresenta e discute propridades de
somatórios.

As seções seguintes apresentam exemplo de problemas simples e suas
soluções, para os quais a eficiência é analisada usando o roteiro
acima. Um método simples de substituição para obtenção de uma fórmula
geral que simplifique a relação de recorrência é apresentado por meio
desses exemplos. Vamos chamar o método de {\em
  substitur-para-generalizar\/}.

\section{Número de Dígitos}
\label{numero-de-digitos}

O problema é determinar o número de dígitos de um número em uma dada
base usada para representação desse número. O número e a base são
dados de entrada.

\subsection{Versão funcional}

A versão funcional é apresentada em Haskell a seguir:

\begin{center}
\begin{tabular}{l}
\begin{hask}{numDigs}{\divisao}
numDigs x b 
  | x < b     = 1
  | otherwise = 1 + numDigs (x `div` b)
\end{hask}
\end{tabular}
\end{center}

A variável que representa o tamanho dos dados de entrada é igual a
$n$.  A variação do tempo de execução $T(n)$ é dada por (considerando
como $k$ uma constante igual ao tempo gasto pela operação de somar 1 a
um valor qualquer mais o tempo gasto pela operação de comparar o
argumento \inh{x} com \inh{b}):

 \[ \begin{array}{lll}
       T(n) & = 0                      & \text{ se } n < \inh{b}\\
       T(n) & = T(n `div` \inh{b}) + k & \text{ caso contrário}
    \end{array}
 \]
Vamos considerar que $n$ é uma potência de \inh{b} --- isto é, $n =
\inh{b}^i$, para algum $i\geq 0$. Essa consideração é baseada na {\em
  regra-de-crescimento-suave}, descrita sucintamente na nota no final
dessa seção (para mais detalhes, veja o Apêndice
\ref{relacoes-de-recorrencia}).

Para $i\geq b$, obtemos:  
 \[ \begin{array}{ll}
       T(\inh{b}^i) & = T(\inh{b}^{i-1}) + k \\
              & = T(\inh{b}^{i-2}) + (2 \times k) \\
              & \ldots
    \end{array}
 \]
Para $n=\inh{b}^i$, obtemos $T(\inh{b}^i) = T(\inh{b}^0) + (i\times k) = i\times k$.  

Portanto, $T(n) = log_{\inh{b}} (i\times k)$ e portanto $T(n) \asymp lg n$.

\HRule
{\em Nota\/}: 

Uma função $f$ é {\em eventualmente não-decrescente\/} se existe $n_0$
tal que $f(n_2) \geq f(n_1)$, para todo $n_2 > n_1 \geq n_0$ no
domínio de $f$.

Uma função $f$ sobre os naturais {\em cresce suavemente\/} se é
eventualmente não-decrescente e $f(2n) \asymp f(n)$.

Funções logarítmicas, polinomiais e combinações lineares de logaritmos
e polinômios são todas funções que crescem suavemente.  Por exemplo, a
função $f$ definida por $f(n) = n lg n$ cresce suavemente, pois:
$f(2n) = 2n lg (2n) = 2n (lg 2 + lg n) = (2 lg 2)n + 2n lg n \asymp n lg n$. 

Funções exponenciais com base maior que 1 e fatoriais não crescem
suavemente. Por exemplo, a função $f$ definida por $f(n) = 2^n$ não
cresce suavemente, pois $f(2n) = 2^{2n} = 4^n \not\asymp 2^n$.
  
Não é difícil mostrar que, para toda função $f$ que cresce suavemente
e para todo $k\geq 2$, temos: $f(kn) \asymp f(n)$. 

A regra-de-crescimento-suave especifica: se $f$ é uma função
eventualmente não-decrescente, $g$ cresce suavemente e $f(n)\asymp g(n)$ 
para valores de $n$ que são potências de $b$, onde $b\geq 2$,
então $f(n) \asymp g(n)$.

\HRule

\subsection{Versão imperativa}

A versão imperativa é similar, usando um comando de repetição em vez
de recursão:

\begin{center}
\begin{tabular}{l}
\begin{alg}{numDigs}{\divisao}
numDigs (n,b)
  numD = 0
  while (n > b) 
       numD = numD + 1
       n = n / b
\end{alg}
\end{tabular}
\end{center}

A expressão-determinante-da-eficiência é igual a $m \times \Theta(1)$,
onde $m$ é o número de vezes que o comando de repetição é executado e
$\Theta(1)$ expressa o tempo gasto nos comandos internos ao comando de
repetição. Como a variável \ina{n} recebe, a cada repetição, o valor
do quociente da divisão do valor de \ina{n} (anterior à atribuição)
por \ina{b}, obtemos: 
 $T(\ina{n}) \asymp m \asymp log_{\ina{b}} \ina{n} \asymp lg \ina{n}$.

Note que $T(\ina{n}) \asymp lg \ina{n}$ para qualquer base \ina{b}.

Note também que $T(\ina{n})$ (e o número de repetições no \while)
aumenta logaritmicamente com um aumento (linear) no {\em valor\/} de
\ina{n}, mas aumenta linearmente com um aumento no número de dígitos
de \ina{n} (o valor de \ina{n} aumenta exponencialmente com um
aumento no número de dígitos de \ina{n}).

\section{Maior Elemento}
\label{maior-elemento}

Considere o problema de encontrar o maior elemento de uma lista. 

A versão funcional apresentada abaixo simplesmente usa \foldl'
(definida em \inh{Data.List}):

\begin{center}
\begin{tabular}{l}
\begin{hask}{maxElem}{\decremento}
maxElem :: Ord a => [a] -> a
maxElem (a:x) = foldl' max a x
\end{hask}
\end{tabular}
\end{center}

A função \inh{max}, definida no prelúdio de Haskell, retorna o maior
entre dois valores, passados como parâmetros:

\begin{center}
\begin{tabular}{l}
\begin{hask}{max}{\definicao}
max :: Ord a => a -> a -> a
max a b 
  | a <= b    = b
  | otherwise = a
\end{hask}
\end{tabular}
\end{center}
 
A função \inh{foldl} é definida no prelúdio de Haskell como a seguir:

\begin{center}
\begin{tabular}{l}
\begin{hask}{foldl}{\decremento}
foldl f z []    = z
foldl f z (a:x) = foldl f (f z a) 
\end{hask}
\end{tabular}
\end{center}

A função \inh{foldl}, aplicada a uma função binária \inh{f}, um valor
inicial \inh{z} e uma lista, ``reduz'' (em geral, mas mais
precisamente transforma) a lista usando a função \inh{f} da esquerda
para a direita (daí o nome \inh{foldl}: o {\it l\/} é de {\em
  {\underline{l}eft\/}}, em português {\em esquerda\/}):

  \[  \inh{foldl f z} [e_1, e_2, \ldots, e_n]
      \inh{ == } 
         (\ldots ((\inh{z `f`} e_1 ) \inh{`f`} e_2) \inh{`f`}\ldots) \inh{`f`}
                   e_n
  \]

A função \inh{foldl'} se comporta de modo similar a \inh{foldl}, mas é
``menos preguiçosa'': \inh{foldl' f} força a avaliação de \inh{f}, de
modo que, \inh{z `f`} $e_1$ é avaliado antes que a expressão $(\inh{z
  `f`} e_1) \inh{`f`} e_2$ seja formada, e assim sucessivamente. Com
\inh{foldl}, toda a expressão $(\ldots ((\inh{z `f`} e_1) \inh{`f`}
e_2) \inh{`f`}\ldots) \inh{`f`} e_n$ é construída antes da avaliação
de $\inh{z `f`} e_1$. O uso de \inh{foldl'} é adequado quando a função
\inh{f} é estrita (ou seja, quando \inh{f} não é preguiçosa), ou,
ainda, quando a avaliação de $\inh{z `f`} e_1$ requer a avaliação de
$e_1$.

\HRule
{\em Nota\/}: 

Uma função $f$ é dita estrita se (escrevendo sucintamente) $f\: \bot =
\bot$, ou seja: quando o resultado de aplicar $f$ a um argumento que
fica em ciclo infinito faz com que a chamada a $f$ fique em ciclo
infinito. O valor $\bot$ é usado para indicar ``ciclo infinito'', e
também ocorrência de erro devido a recursos, em quantidade finita,
serem consumidos para avaliação, durante a execução. 

Em Haskell, ao contrário da grande maioria das linguagens de
programação, a estratégia de avaliação de expressões é ``preguiçosa''
(em inglês, ``lazy''). Isso significa que o argumento para uma função
não é avaliado quando a expressão é chamada, mas simplesmente
substituído pelo parâmetro no corpo da função, para posterior
avaliação, se necessário. Além disso, se for necessária, a avaliação
do argumento só é feita uma única vez, na estratégia de avaliação
preguiçosa; nas outras vezes em que o argumento for usado, é usado o
valor resultante da avaliação feita na primeira vez.

Na maioria das linguagens de programação, a estratégia de avaliação de
expressões é ``gulosa'' (em inglês, ``eager''). Nessa estratégia, o
argumento é avaliado antes de uma chamada à função. Essa estratégia
faz com que todas as funções sejam estritas.

As diferenças resultantes do uso de estratégias de avaliação
preguiçosa e gulosa estão fora do escopo deste livro. Para os
programas que vamos analisar neste livro, a análise de eficiência de
programas com estratégia de avaliação preguiçosa não difere da análise
com a estratégia de avaliação gulosa. 

\HRule
{\em Nota\/}: 

Este livro provê uma introdução a análise do tempo de execução de
programas. A complexidade de tempo de programas usando avaliação
preguiçosa é sempre igual ou menor (sendo $\preceq$ a ordem de
comparação) que a complexidade usando avaliação estrita. Isso ocorre
porque a avaliação de cada expressão pode não ser realizada e, se for
realizada, a avaliação é feita apenas uma vez. A diferença mais
significativa entre as duas estratégias é relativa não ao tempo mas ao
espaço gasto durante a execução de programas: a estratégia de
avaliação preguiçosa pode requerer mais espaço durante a execução,
justamente para armazenamento de informações para avaliação de
expressões que não foram, mas eventualmente poderão ter que ser,
avaliadas. Isso pode causar diminuição da constante usada na
complexidade de tempo do programa, e pode fazer com que que o espaço
necessário para avaliação do programa seja maior do que o que foi
alocado para essa execução (pode ocorrer, por exemplo, o que é chamado
de ``estouro de pilha''). Por outro lado, há casos em que ocorre o
contrário. A estratégia gulosa permite o uso e manipulação de
estruturas de dados de tamanho maior do que o que seria possível com a
estratégia de avaliação gulosa, e mesmo estruturaa de dados de tamanho
ilimitado. Por exemplo, considere a função \inh{f n = take n [1..]}.
Apesar de usar uma lista de tamanho ilimitado, ela tem o mesmo
comportamento, em termos de significado e em termos do tempo de
execução gasto para sua execução, do que a função \inh{g n = [1..n]}.

\HRule

No entanto, vale observar que o uso de \inh{foldl'} neste exemplo não
altera a complexidade de tempo, mas pode afetar bastante o espaço
necessário e a constante de proporcionalidade da função que expressa o
tempo de execução.

\HRule

A relação de recorrência é 

 \begin{equation}
    T(n) = T(n-1) + k 
    \label{recorrencia1}
 \end{equation}
onde $n$ é o tamanho (número de elementos) da lista e $k$ é uma
constante que expressa o tempo de execução da aplicação da função
$\max$ a dois valores inteiros. No caso base, temos $T(0) = 0$.

Essa relação de recorrência tem solução fácil. Vamos mostrar sua
solução pelo {\em
  método-de-substituição-para-eliminação-da-recursão\/} ilustrado a
seguir. 

Temos: $T(n-1) = T(n-2) + k$. Substituindo $T(n-1)$ na relação de
recorrência (\ref{recorrencia1}), obtemos: 
  $T(n) = (T(n-2) + k) + k = T(n-2) + 2\times k$. 
É fácil ver que, para todo $i=1,\ldots,n$, temos:
$T(n) = T(n-i) + i\times k$. Para $i=n$, temos: $T(n) = n\times k$ e,
portanto, $T(n) = \Theta(n)$.

Note que não é preciso o método acima (de substituir-para-generalizar)
para concluir que $T(n) = n\times k$; basta raciocinar sobre a
definição de $T$:

    \[ \begin{array}[t]{ll}
         T(n) = 0          & \text{ se } n = 0\\
         T(n) = T(n-1) + k & \text{ caso contrário}
       \end{array}
    \]

A expressão que define $T(n)$ é igual a 0 quando $n$ é igual a 0 e
aumenta de $k$ qundo $n$ aumenta de 1; ou seja, é uma definição de
$n\times k$.

O que chamamos de expressão-determinante-da-eficiência pode ser
expresso por $T(n-1) + k$, ou (depois de resolvida a relação de
recorrência) por $n\times k$.

A complexidade é, assim, a mesma da pesquisa sequencial em uma lista:
$O(n)$ no pior caso. Informalmente, o raciocínio pode ser similar ao
seguinte: o tempo de execução é $O(n)$ pois o custo das operações
realizadas em chamada recursiva é constante ($O(1)$) e o número de
chamadas recursivas é $O(n)$ (cresce linearmente com $n$).

\subsection{Versão imperativa}

A versão imperativa considera por simplicidade um número máximo de
elementos $n > 0$ para determinar o maior dos elementos armazenados em
um arranjo (com índices de :

\begin{center}
\begin{tabular}{l}
\begin{alg}{maxElem}{\decremento}
maxElem (A) 
  max §$\from$§ A[0]
  for i §$\from$§ 1 to n-1 
     if A[i] > max
        max §$\from$§ A[i]
  return max
\end{alg}
\end{tabular}
\end{center}

A expressão-determinante-da-eficiência é igual a $\ina{n} \times
\Theta(1)$, onde $\Theta(1)$ expressa o tempo gasto para comparação e
atribuição (nos comandos de seleção e atribuição internos ao comando
de repetição). Obtemos: $T(\ina{n}) \asymp \sum_{i=1}{\ina{n}} \Theta(1)
\asymp \ina{n}$.

\section{Unicidade}
\label{unicidade}

Considere o problema de verificar se todos os elementos de uma dada
sequência de elementos são distintos. 

\subsection{Versão funcional}

A versão funcional é apresentada a seguir:

\begin{center}
\begin{tabular}{l}
\begin{hask}{allUnique,chkUnique}{\decremento}
allUnique :: Eq a => [a] -> Bool
allUnique = fst . foldr chkUnique (True,[])

chkUnique :: Eq a => a -> (Bool,[a]) -> (Bool,[a])
chkUnique a (True,x) 
 | a `elem` x   = (False,undefined)
 | otherwise    = (True, a:x)
chkUnique _ r   = r
\end{hask}
\end{tabular}
\end{center}

A função \inh{chkUnique} usa um par $(b,x)$, onde $b$ indica unicidade
(de todos os elementos testados até agora), e $x$ representa a lista
dos elementos já testados. A unicidade de cada elemento $a$ da lista é
testada com relação à lista dos já testados $x$: se $a$ não pertence a
$x$, então a unicidade é preservada e $a$ é adicionado a $x$, caso
contrário não há unicidade e a verificação pode ser interrompida.

A função \inh{allUnique} percorre a lista para produzir o resultado
desejado, usando \inh{chkUnique}.

A relação de recorrência é 

 \[ T(n) = T(n-1) + f(n) + k \]
onde \inh{n} é o tamanho (número de elementos) da lista, \inh{f} é a
função de complexidade de tempo de \inh{chkUnique} e $k$ é uma
constante que expressa o tempo de execução de \inh{fst}.  Temos:
$\inh{f}(\inh{n}) = \inh{n} + k'$, onde $k'$ é uma constante que
expressa o tempo de execução \inh{(:) a x}. Obtemos então:

  \[ T(\inh{n}) = T(\inh{n}-i) + i \times (\inh{n} + k + k') \]
Com $i=\inh{n}$, obtemos: 

  \[ T(\inh{n}) = T(0) + \inh{n}^2 + \Theta(\inh{n}) \]
Como $T(0)$ é constante, obtemos: 

  \[ T(\inh{n}) \asymp \inh{n}^2 \]

\subsection{Versão imperativa}

Na versão imperativa por simplicidade vamos considerar que a sequência
está armazenada em um arranjo de tamanho \ina{n}. O programa é
mostrado a seguir:

\begin{center}
\begin{tabular}{l}
\begin{alg}{allUnique}{\decremento}
allUnique(A) 
  for i §$\from$§ 0 to n-2 do
    for j §$\from$§ i+1 to n-1 do
        if A[i] == A[j] return false
\end{alg}
\end{tabular}
\end{center}

Temos: $T(\ina{n}) = \sum_{i=0}^{\ina{n}-2} \sum_{j=i+1}^{\ina{n}-1} k$,
onde $k$ é o custo de comparação de dois elementos do arranjo.

Simplificando, obtemos: 

  \[ \begin{array}{lll} 
      T(\ina{n}) & = & \sum_{i=0}^{\ina{n}-2} (k \times (\ina{n} - i - 1)) \\
                 & = & (\ina{n}-1) + (\ina{n}-2) + \ldots + 1 \\
                 & = & (\ina{n}-1) \times \ina{n} / 2
% didn't use \frac 'cause hevea doesn't like \ina{n} inside \frac... :-(
     \end{array}
  \]
Ou seja, $T(\ina{n}) \asymp \ina{n}^2$.

\input{multiplicacao-de-matrizes}

\input{numeros-de-fibonacci}

\section{Exercícios}

\begin{enumerate}

\item Escreva uma função com argumentos $x$ e $n$, sendo $n$ um
  inteiro positivo, e retorne o valor de $x^n$ ($x$ elevado à
  $n$-ésima potência) de modo que a complexidade de tempo da função
  seja logarítmica ($O(lg n)$).

Sua função deve levar em conta que, se $n$ é par, então $x^n$ é igual
ao quadrado de $x^{n/2}$ e, se $n$ é ímpar, então $x^n$ é igual ao
produto de $x$ por $x^{n-1}$.

Escreva duas versões diferentes, uma que usa recursão e outra que usa
comando de repetição.

Prove que as duas definições têm complexidade de tempo $O(lg n)$.

\end{enumerate}

